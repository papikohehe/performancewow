# app.py

import io
import re
import sys
import warnings
from dataclasses import dataclass
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import streamlit as st


# -----------------------------
# Page config
# -----------------------------
st.set_page_config(
    page_title="‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û",
    page_icon="üìà",
    layout="wide",
    initial_sidebar_state="expanded",
)


# -----------------------------
# Helpers
# -----------------------------
REQUIRED_COLS = ["Time (day)", "Hour", "Employee ID", "Name", "Processed Count"]

NORMALIZE_MAP = {
    # Date/Day
    "time (day)": "Time (day)", "time": "Time (day)", "date": "Time (day)", "day": "Time (day)", "date (day)": "Time (day)", "‡∏ß‡∏±‡∏ô": "Time (day)",
    # Hour
    "hour": "Hour", "hours": "Hour", "time (hour)": "Hour", "start hour": "Hour", "hr": "Hour", "‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á": "Hour",
    # Employee ID
    "employee id": "Employee ID", "emp id": "Employee ID", "employeeid": "Employee ID", "id": "Employee ID", "‡∏£‡∏´‡∏±‡∏™‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô": "Employee ID",
    # Name
    "name": "Name", "employee": "Name", "employee name": "Name", "‡∏ä‡∏∑‡πà‡∏≠": "Name",
    # Processed Count
    "processed count": "Processed Count", "processed": "Processed Count", "count": "Processed Count", "work": "Processed Count",
    "works": "Processed Count", "items": "Processed Count", "workload": "Processed Count", "items processed": "Processed Count",
    "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô": "Processed Count", "‡∏¢‡∏≠‡∏î": "Processed Count", "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥": "Processed Count",
}


@st.cache_data(show_spinner=False)
def read_csv_safely(file, by_position=False) -> pd.DataFrame:
    """Robust CSV loader. Can read by position or by header name."""
    read_params = {'header': None, 'skiprows': 1} if by_position else {'header': 0}
    
    for sep in [",", "\t", ";"]:
        for enc in ["utf-8", "utf-8-sig", "latin-1"]:
            try:
                file.seek(0)
                df = pd.read_csv(file, sep=sep, encoding=enc, **read_params)
                if df.shape[1] >= 3:
                    return df
            except Exception:
                continue
    file.seek(0)
    return pd.read_csv(file, **read_params)

def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    new_cols = [str(c).strip().lower() for c in df.columns]
    df.columns = [NORMALIZE_MAP.get(c, c) for c in new_cols]
    return df

def coerce_schema(df: pd.DataFrame, filename: str = "") -> Tuple[pd.DataFrame, List[str]]:
    """Cleans and validates the main performance data."""
    issues = []
    df = normalize_columns(df.copy())

    if "Name" in df.columns:
        df = df[df["Name"] != "-"].copy()

    missing = [c for c in REQUIRED_COLS if c not in df.columns]
    if missing:
        issues.append(f"Missing columns: {missing}")

    if "Time (day)" in df.columns:
        df.dropna(subset=["Time (day)"], inplace=True)
        if df.empty:
            return df, issues
        
        reconstructed, day_numbers = False, pd.to_numeric(df["Time (day)"], errors='coerce')
        is_days = day_numbers.notna().all() and day_numbers.min() >= 1 and day_numbers.max() <= 31

        if is_days:
            match = re.search(r"(\d{4})-(\d{2})", filename)
            if match:
                year, month = match.groups()
                df['Time (day)'] = pd.to_datetime({'year': int(year), 'month': int(month), 'day': day_numbers}, errors='coerce')
                reconstructed = True
        
        if not reconstructed:
            df['Time (day)'] = pd.to_datetime(df["Time (day)"], errors="coerce")

        df["Time (day)"] = df["Time (day)"].dt.date if pd.api.types.is_datetime64_any_dtype(df["Time (day)"]) else pd.NaT

    if "Hour" in df.columns:
        def _to_hour(x):
            try:
                s = str(x).strip().split(":")[0]
                h = int(float(s))
                return h if 0 <= h <= 23 else np.nan
            except: return np.nan
        df["Hour"] = df["Hour"].apply(_to_hour)
    
    for col in ["Employee ID", "Name"]:
        if col in df.columns:
            df[col] = df[col].astype(str).str.replace(r'\.0$', '', regex=True)

    if "Processed Count" in df.columns:
        df["Processed Count"] = pd.to_numeric(df["Processed Count"], errors="coerce").fillna(0)
        df.loc[df["Processed Count"] < 0, "Processed Count"] = 0

    return df, issues

def clean_percentage(series: pd.Series) -> pd.Series:
    return pd.to_numeric(series.astype(str).str.replace('%', ''), errors='coerce') / 100

@st.cache_data(show_spinner=False)
def process_qa_data(file) -> pd.DataFrame:
    try:
        df = read_csv_safely(file, by_position=False)
        if df.empty: return pd.DataFrame()
    except Exception as e:
        st.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå QA ‡πÑ‡∏î‡πâ: {e}"); return pd.DataFrame()

    df['Time (day)'].replace(r'^\s*$', np.nan, regex=True, inplace=True)
    df['Time (day)'].ffill(inplace=True)
    df.dropna(subset=['Employee ID', 'Name', 'Time (day)'], inplace=True)
    df = df[df['Employee ID'] != '-'].copy()
    
    filename = getattr(file, 'name', '')
    match = re.search(r"(\d{4})-(\d{2})", filename)
    if match:
        year, month = match.groups()
        day_col = pd.to_numeric(df["Time (day)"], errors='coerce')
        df['Date'] = pd.to_datetime({'year': int(year), 'month': int(month), 'day': day_col}, errors='coerce').dt.date
    else:
        df['Date'] = pd.NaT

    df.dropna(subset=['Date'], inplace=True)
    if df.empty: return pd.DataFrame()

    accuracy_cols = ['Total Accuracy', 'Approved Accuracy', 'Declined Accuracy']
    for col in accuracy_cols:
        if col in df.columns:
            df[col] = clean_percentage(df[col])
            
    final_cols = ['Date', 'Employee ID', 'Name'] + [c for c in accuracy_cols if c in df.columns]
    return df[final_cols]

def deduplicate_rows(df: pd.DataFrame) -> pd.DataFrame:
    key_cols = [c for c in REQUIRED_COLS if c in df.columns]
    if not key_cols: return df
    return df.groupby(key_cols, dropna=False, as_index=False)["Processed Count"].sum()

def compute_aggregates(df, count_hour_when_processed_gt_zero=True):
    df = df.copy()
    if not all(c in df.columns for c in ["Hour", "Time (day)"]):
        return df, pd.DataFrame(), pd.DataFrame()

    worked_mask = (df["Processed Count"] > 0) if count_hour_when_processed_gt_zero else ~df["Hour"].isna()
    df["WorkedHourFlag"] = worked_mask.astype(int)

    emp_daily = df.groupby(["Time (day)", "Employee ID", "Name"], dropna=False).agg(
        Works=("Processed Count", "sum"), Hours=("WorkedHourFlag", "sum")
    ).reset_index()
    emp_daily["WPH"] = emp_daily["Works"] / emp_daily["Hours"].replace(0, np.nan)

    emp_summary = emp_daily.groupby(["Employee ID", "Name"], dropna=False).agg(
        Days=("Time (day)", "nunique"), Hours=("Hours", "sum"), Works=("Works", "sum"), AvgWPH=("WPH", "mean")
    ).reset_index()
    emp_summary["WPH"] = emp_summary["Works"] / emp_summary["Hours"].replace(0, np.nan)
    return df, emp_daily, emp_summary

def add_topn_bar(df: pd.DataFrame, metric: str, top_n: int, title: str, ascending=False, text_auto_format=True):
    if df.empty or metric not in df.columns:
        st.info("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á"); return
    show = df.dropna(subset=[metric]).sort_values(metric, ascending=ascending).head(top_n)
    fig = px.bar(show, x="Name", y=metric, hover_data=["Employee ID"], title=title, text_auto=text_auto_format)
    fig.update_traces(textangle=0, textposition="outside", cliponaxis=False)
    st.plotly_chart(fig, use_container_width=True)
    st.dataframe(show.reset_index(drop=True))

def kpi_block(total_works, total_hours, avg_wph, employees, days):
    c1, c2, c3, c4, c5 = st.columns(5)
    c1.metric("‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", f"{int(total_works):,}")
    c2.metric("‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", f"{int(total_hours):,}")
    c3.metric("‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á (‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢)", f"{avg_wph:,.2f}" if pd.notna(avg_wph) else "‚Äî")
    c4.metric("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô", f"{employees:,}")
    c5.metric("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏ô", f"{days:,}")

def download_csv_button(df, filename, label):
    st.download_button(label=label, data=df.to_csv(index=False).encode("utf-8"), file_name=filename, mime="text/csv")

# UI
st.title("üìà ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û")
st.markdown("""‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå: **Time (day)**, **Hour**, **Employee ID**, **Name**, **Processed Count**""")
with st.sidebar:
    st.header("‚öôÔ∏è Settings"); hour_count_mode = st.radio("‡∏ô‡∏±‡∏ö‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‚Ä¶", ["‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Processed Count > 0 (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)", "‡∏°‡∏µ‡πÅ‡∏ñ‡∏ß‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡∏π‡πà (‡πÅ‡∏°‡πâ Processed Count = 0)"])
    count_when_gt_zero = hour_count_mode.startswith("‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"); min_hours_threshold = st.slider("‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö WPH", 1, 40, 8, 1)
    top_n = st.slider("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á", 3, 50, 10, 1); st.divider(); st.caption("‡∏ï‡∏±‡∏ß‡∏Å‡∏£‡∏≠‡∏á (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏î‡πâ)")
    date_filter_on = st.checkbox("‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà"); employee_search = st.text_input("‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô"); st.divider()
    use_sample = st.checkbox("‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á")

# Load Performance Data
st.header("1. Performance Report (‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏á‡∏≤‡∏ô)")
uploaded_files = st.file_uploader("‡∏ß‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå Performance Report ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà", type=["csv"], accept_multiple_files=True)
dfs, issues_all = [], []

if use_sample and not uploaded_files:
    rng = np.random.default_rng(42); dates = pd.date_range("2025-08-01", periods=10, freq="D").date
    rows = [[d, h, eid, nm, int(rng.poisson(8)) if rng.random() < 0.8 else 0]
            for d in dates for h in range(9, 19) for eid, nm in [("E001", "Alice"), ("E002", "Bob"), ("E003", "Chai"), ("E004", "Dao")]]
    sample = pd.DataFrame(rows, columns=REQUIRED_COLS)
    df0, issues = coerce_schema(sample, "sample-2025-08.csv")
    dfs.append(deduplicate_rows(df0)); issues_all.extend(issues)

for f in uploaded_files:
    try:
        raw = read_csv_safely(f, by_position=False)
        df0, issues = coerce_schema(raw, getattr(f, 'name', ''))
        dfs.append(deduplicate_rows(df0)); issues_all.extend(issues)
    except Exception as e:
        st.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå {getattr(f, 'name', 'file')}: {e}")

if not dfs:
    st.info("üëÜ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Performance Report ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô **‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô")
    st.stop()

df = pd.concat(dfs, ignore_index=True)
if issues_all:
    with st.expander("‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Performance"): st.warning("\n".join(set(issues_all)))

df.dropna(subset=['Time (day)'], inplace=True)
if df.empty:
    st.error("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Performance ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì"); st.stop()

# Main App Logic
if date_filter_on:
    min_d, max_d = df["Time (day)"].min(), df["Time (day)"].max()
    d1, d2 = st.slider("‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà", min_d, max_d, (min_d, max_d))
    df = df[df["Time (day)"].between(d1, d2)].copy()
if employee_search.strip():
    df = df[df["Name"].str.contains(employee_search.strip(), case=False, na=False)].copy()

emp_hourly, emp_daily, emp_summary = compute_aggregates(df, count_when_gt_zero)
if emp_daily.empty:
    st.warning("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì"); st.stop()
    
total_works, total_hours = emp_daily["Works"].sum(), emp_daily["Hours"].sum()
avg_wph = total_works / total_hours if total_hours > 0 else np.nan
employees, days = emp_summary.shape[0], emp_daily["Time (day)"].nunique()

st.subheader("‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏° Performance")
kpi_block(total_works, total_hours, avg_wph, employees, days)

# Tabs
tab_perf, tab_time, tab_drill, tab_anom, tab_pivot, tab_qa, tab_export = st.tabs(
    ["üèÜ Leaderboards", "üìÖ Time Analysis", "üë§ Employee Drilldown", "‚ö†Ô∏è Anomalies", "üìä Pivots", "üéØ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥", "üì• Export"]
)
with tab_perf:
    st.subheader("Leaderboards")
    st.caption(f"‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥ **{min_hours_threshold}** ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö WPH")
    eligible = emp_summary[emp_summary["Hours"] >= min_hours_threshold].copy()
    c1, c2, c3 = st.columns(3, gap="large")
    with c1: add_topn_bar(emp_summary, "Works", top_n, "Top by Total Works")
    with c2:
        if eligible.empty: st.info("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö WPH ranking")
        else: add_topn_bar(eligible, "WPH", top_n, "Top by Works per Hour (WPH)", text_auto_format='.2f')
    with c3: add_topn_bar(emp_summary, "Hours", top_n, "Top by Hours Worked")

with tab_time:
    st.subheader("Time Analysis")
    fig1 = px.line(emp_hourly.groupby("Hour")["Processed Count"].sum().reset_index(), x="Hour", y="Processed Count", markers=True, title="Throughput by Hour of Day")
    st.plotly_chart(fig1, use_container_width=True)
    thr, cov = heatmap_data(emp_hourly)
    if not thr.empty:
        st.markdown("**Throughput Heatmap (Day √ó Hour)**"); fig2 = px.imshow(thr, aspect="auto", labels=dict(color="Works"))
        st.plotly_chart(fig2, use_container_width=True)

with tab_drill:
    st.subheader("Employee Drilldown")
    names = sorted(emp_summary["Name"].unique())
    picked = st.selectbox("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô", options=names)
    if picked:
        emp_ids = emp_summary.loc[emp_summary["Name"] == picked, "Employee ID"].unique()
        emp_id = st.selectbox("Multiple IDs found, pick one:", emp_ids) if len(emp_ids) > 1 else emp_ids[0]
        dfd = emp_daily[emp_daily["Employee ID"] == emp_id].copy()
        if not dfd.empty:
            w, h = dfd["Works"].sum(), dfd["Hours"].sum()
            c1,c2,c3=st.columns(3);c1.metric("Works",f"{w:,}");c2.metric("Hours",f"{h:,}");c3.metric("WPH",f"{(w/h if h>0 else 0):,.2f}")
            st.plotly_chart(px.line(dfd, x="Time (day)", y="Works", markers=True, title=f"Daily Works ‚Äî {picked}"), use_container_width=True)

with tab_anom:
    st.subheader("Anomalies")
    if emp_daily.shape[0] < 2: st.info("No data for anomaly detection.")
    else:
        z_thr = st.slider("Z-score threshold", 1.0, 4.0, 2.0, 0.5)
        anom = emp_daily.groupby("Name", group_keys=False).apply(lambda g: g.assign(WorksZ=zscore(g["Works"])))
        flagged = anom[anom["WorksZ"].abs() >= z_thr].copy()
        st.caption("An anomaly is a day where an employee's daily 'Works' deviates from their own mean.")
        st.dataframe(flagged.sort_values(["Name", "Time (day)"]))

with tab_pivot:
    st.subheader("Pivots")
    piv = emp_hourly.pivot_table(index="Time (day)", columns="Name", values="Processed Count", aggfunc="sum").fillna(0)
    st.markdown("**Daily √ó Employee ‚Äî Works**"); st.dataframe(piv)

with tab_qa:
    st.subheader("üéØ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ (QA Accuracy)")
    qa_file = st.file_uploader("‡∏ß‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå QA Report ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà", type=["csv"])
    if qa_file:
        qa_df = process_qa_data(qa_file)
        if not qa_df.empty:
            st.metric("‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ Total Accuracy ‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°", f"{qa_df['Total Accuracy'].mean():.2%}")
            acc_summary = qa_df.groupby(["Employee ID", "Name"], as_index=False)["Total Accuracy"].mean()
            add_topn_bar(acc_summary, "Total Accuracy", top_n, "Top by Average Total Accuracy", text_auto_format='.2%')
            with st.expander("‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• QA ‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡∏µ‡∏ô‡πÅ‡∏•‡πâ‡∏ß"): st.dataframe(qa_df)
        else:
            st.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå QA ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏î‡πâ")
    else:
        st.info("‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå QA Report ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥")

with tab_export:
    st.subheader("Export Data")
    c1,c2,c3=st.columns(3)
    with c1: download_csv_button(emp_summary, "employee_summary.csv", "‚¨áÔ∏è Employee Summary")
    with c2: download_csv_button(emp_daily, "employee_daily.csv", "‚¨áÔ∏è Daily Aggregates")
    with c3: download_csv_button(emp_hourly, "employee_hourly.csv", "‚¨áÔ∏è Granular Hourly Records")
