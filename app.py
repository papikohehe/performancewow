# app.py

import io
import sys
import warnings
from dataclasses import dataclass
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import streamlit as st


# -----------------------------
# Page config
# -----------------------------
st.set_page_config(
    page_title="‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û",
    page_icon="üìà",
    layout="wide",
    initial_sidebar_state="expanded",
)


# -----------------------------
# Helpers
# -----------------------------
REQUIRED_COLS = ["Time (day)", "Hour", "Employee ID", "Name", "Processed Count"]

NORMALIZE_MAP = {
    # Date/Day
    "time (day)": "Time (day)",
    "time": "Time (day)",
    "date": "Time (day)",
    "day": "Time (day)",
    "date (day)": "Time (day)",
    # Hour
    "hour": "Hour",
    "hours": "Hour",
    "time (hour)": "Hour",
    "start hour": "Hour",
    "hr": "Hour",
    # Employee ID
    "employee id": "Employee ID",
    "emp id": "Employee ID",
    "employeeid": "Employee ID",
    "id": "Employee ID",
    # Name
    "name": "Name",
    "employee": "Name",
    "employee name": "Name",
    # Processed Count
    "processed count": "Processed Count",
    "processed": "Processed Count",
    "count": "Processed Count",
    "work": "Processed Count",
    "works": "Processed Count",
    "items": "Processed Count",
}


@st.cache_data(show_spinner=False)
def read_csv_safely(file) -> pd.DataFrame:
    """Robust CSV loader that tries utf-8, then latin-1, with comma or tab separators."""
    # Try comma with utf-8
    for sep in [",", "\t", ";"]:
        for enc in ["utf-8", "utf-8-sig", "latin-1"]:
            try:
                file.seek(0)
                df = pd.read_csv(file, sep=sep, encoding=enc)
                if df.shape[1] >= 3:
                    return df
            except Exception:
                continue
    # Fallback to pandas default
    file.seek(0)
    return pd.read_csv(file)


def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    new_cols = []
    for c in df.columns:
        key = str(c).strip().lower()
        new_cols.append(NORMALIZE_MAP.get(key, c))
    df.columns = new_cols
    return df


def coerce_schema(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:
    """Ensure required columns exist and coerce dtypes."""
    issues = []
    df = normalize_columns(df.copy())

    missing = [c for c in REQUIRED_COLS if c not in df.columns]
    if missing:
        issues.append(f"Missing columns: {missing}")

    # Coerce Time (day)
    if "Time (day)" in df.columns:
        try:
            df["Time (day)"] = pd.to_datetime(df["Time (day)"], errors="coerce").dt.date
        except Exception as e:
            issues.append(f"Failed to parse 'Time (day)' as dates: {e}")
            df["Time (day)"] = pd.NaT

    # Coerce Hour to int 0-23 where possible
    if "Hour" in df.columns:
        def _to_hour(x):
            try:
                # Handle "09", "9", "9:00"
                s = str(x).strip()
                if ":" in s:
                    s = s.split(":")[0]
                h = int(float(s))
                if 0 <= h <= 23:
                    return h
            except Exception:
                return np.nan
            return np.nan

        df["Hour"] = df["Hour"].apply(_to_hour)
        # If hour strings like "24" appear, clamp to 23
        df.loc[df["Hour"].isna(), "Hour"] = np.nan

    # Employee ID as string
    if "Employee ID" in df.columns:
        df["Employee ID"] = df["Employee ID"].astype(str)

    # Name as string
    if "Name" in df.columns:
        df["Name"] = df["Name"].astype(str)

    # Processed Count as numeric >= 0
    if "Processed Count" in df.columns:
        df["Processed Count"] = pd.to_numeric(df["Processed Count"], errors="coerce")
        neg = (df["Processed Count"] < 0).sum()
        if neg:
            issues.append(f"{neg} rows had negative 'Processed Count' and were set to 0.")
        df["Processed Count"] = df["Processed Count"].fillna(0)
        df.loc[df["Processed Count"] < 0, "Processed Count"] = 0

    return df, issues


def deduplicate_rows(df: pd.DataFrame) -> pd.DataFrame:
    """Combine duplicates by summing Processed Count within the same (day, hour, emp)."""
    key_cols = [c for c in ["Time (day)", "Hour", "Employee ID", "Name"] if c in df.columns]
    if not key_cols or "Processed Count" not in df.columns:
        return df
    g = df.groupby(key_cols, dropna=False, as_index=False)["Processed Count"].sum()
    return g


def compute_aggregates(
    df: pd.DataFrame,
    count_hour_when_processed_gt_zero: bool = True,
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    Returns:
        - emp_hourly: granular per (day, hour, emp)
        - emp_daily: per (day, emp)
        - emp_summary: per employee
    """
    df = df.copy()

    # Fill missing hours if any‚Äîcannot compute hours without Hour column
    if "Hour" not in df.columns or "Time (day)" not in df.columns:
        st.warning("Cannot compute aggregates without 'Time (day)' and 'Hour'.")
        return df, df, df

    # Decide hours worked
    if count_hour_when_processed_gt_zero:
        worked_mask = df["Processed Count"] > 0
    else:
        worked_mask = ~df["Hour"].isna()

    df["WorkedHourFlag"] = worked_mask.astype(int)

    # Per-employee per-day
    daily_keys = ["Time (day)", "Employee ID", "Name"]
    emp_daily = (
        df.groupby(daily_keys, dropna=False)
        .agg(
            Works=("Processed Count", "sum"),
            Hours=("WorkedHourFlag", "sum"),
            UniqueHours=("Hour", pd.Series.nunique),
        )
        .reset_index()
    )
    emp_daily["WPH"] = emp_daily["Works"] / emp_daily["Hours"].replace(0, np.nan)

    # Per-employee summary
    emp_summary = (
        emp_daily.groupby(["Employee ID", "Name"], dropna=False)
        .agg(
            Days=("Time (day)", pd.Series.nunique),
            Hours=("Hours", "sum"),
            UniqueHours=("UniqueHours", "sum"),
            Works=("Works", "sum"),
            AvgWPH=("WPH", "mean"),
            MedianWPH=("WPH", "median"),
            MaxDailyWorks=("Works", "max"),
        )
        .reset_index()
    )
    emp_summary["WPH"] = emp_summary["Works"] / emp_summary["Hours"].replace(0, np.nan)
    emp_summary["WorksPerDay"] = emp_summary["Works"] / emp_summary["Days"].replace(0, np.nan)

    # Back out the granular hourly for completeness
    emp_hourly = df.copy()

    return emp_hourly, emp_daily, emp_summary


def zscore(series: pd.Series) -> pd.Series:
    m = series.mean()
    s = series.std(ddof=0)
    if s == 0 or np.isnan(s):
        return pd.Series([0] * len(series), index=series.index)
    return (series - m) / s


def anomaly_report(emp_daily: pd.DataFrame, z_threshold: float = 2.0) -> pd.DataFrame:
    """Compute z-score on per-employee daily Works to find spikes or dips."""
    if emp_daily.empty:
        return emp_daily

    def _emp_anom(df_emp):
        df_emp = df_emp.sort_values("Time (day)").copy()
        df_emp["WorksZ"] = zscore(df_emp["Works"])
        df_emp["IsAnomaly"] = df_emp["WorksZ"].abs() >= z_threshold
        return df_emp

    out = (
        emp_daily.groupby(["Employee ID", "Name"], group_keys=False)
        .apply(_emp_anom)
        .reset_index(drop=True)
    )
    return out


def heatmap_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """Return two pivot tables: throughput heatmap & staffing coverage heatmap."""
    if df.empty or "Time (day)" not in df.columns or "Hour" not in df.columns:
        return pd.DataFrame(), pd.DataFrame()

    tmp = df.copy()
    tmp["Dow"] = pd.to_datetime(tmp["Time (day)"]).dt.day_name()

    # Throughput by hour x day-of-week
    thr = (
        tmp.groupby(["Dow", "Hour"])["Processed Count"]
        .sum()
        .reset_index()
        .pivot(index="Dow", columns="Hour", values="Processed Count")
        .fillna(0)
    )

    # Coverage: unique employees per hour x day-of-week
    cov = (
        tmp.groupby(["Dow", "Hour"])["Employee ID"]
        .nunique()
        .reset_index()
        .pivot(index="Dow", columns="Hour", values="Employee ID")
        .fillna(0)
    )
    # Sort DOW in conventional order
    dow_order = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]
    thr = thr.reindex(dow_order)
    cov = cov.reindex(dow_order)
    return thr, cov


def add_topn_bar(df: pd.DataFrame, metric: str, top_n: int, title: str):
    if df.empty or metric not in df.columns:
        st.info("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á")
        return
    show = df.dropna(subset=[metric]).sort_values(metric, ascending=False).head(top_n)
    fig = px.bar(show, x="Name", y=metric, hover_data=["Employee ID"], title=title)
    st.plotly_chart(fig, use_container_width=True)
    st.dataframe(show.reset_index(drop=True))


def kpi_block(total_works, total_hours, avg_wph, employees, days):
    c1, c2, c3, c4, c5 = st.columns(5)
    c1.metric("‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", f"{int(total_works):,}")
    c2.metric("‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", f"{int(total_hours):,}")
    c3.metric("‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á (‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢)", f"{avg_wph:,.2f}" if not np.isnan(avg_wph) else "‚Äî")
    c4.metric("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô", f"{employees:,}")
    c5.metric("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏ô", f"{days:,}")


def download_csv_button(df: pd.DataFrame, filename: str, label: str):
    csv = df.to_csv(index=False).encode("utf-8")
    st.download_button(label=label, data=csv, file_name=filename, mime="text/csv")


# -----------------------------
# UI
# -----------------------------
st.title("üìà ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û")

st.markdown(
    """
‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå CSV 1 ‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ ‡πÇ‡∏î‡∏¢‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏î‡∏±‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ:
- **Time (day)** (‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà)
- **Hour** (‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á, 0‚Äì23)
- **Employee ID** (‡∏£‡∏´‡∏±‡∏™‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô)
- **Name** (‡∏ä‡∏∑‡πà‡∏≠‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô)
- **Processed Count** (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥)

‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô‡∏à‡∏∞‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏õ‡∏£‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (‡πÄ‡∏ä‡πà‡∏ô `Date`‚Üí`Time (day)`, `Processed`‚Üí`Processed Count`)
"""
)

with st.sidebar:
    st.header("‚öôÔ∏è ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤")
    st.caption("‡∏à‡∏∞‡∏ô‡∏±‡∏ö‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?")
    hour_count_mode = st.radio(
        "‡∏ô‡∏±‡∏ö‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‚Ä¶",
        options=[
            "‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Processed Count > 0 (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)",
            "‡∏°‡∏µ‡πÅ‡∏ñ‡∏ß‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡∏π‡πà (‡πÅ‡∏°‡πâ Processed Count = 0)",
        ],
        index=0,
    )
    count_when_gt_zero = hour_count_mode.startswith("‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")

    min_hours_threshold = st.slider("‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏ô‡∏•‡∏µ‡∏î‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ö‡∏≠‡∏£‡πå‡∏î WPH", 1, 40, 8, 1)
    top_n = st.slider("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á", 3, 50, 10, 1)

    st.divider()
    st.caption("‡∏ï‡∏±‡∏ß‡∏Å‡∏£‡∏≠‡∏á (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏î‡πâ)")
    date_filter_on = st.checkbox("‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà", value=False)
    employee_search = st.text_input("‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô (‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö)", value="")

    st.divider()
    st.caption("‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
    use_sample = st.checkbox("‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á (‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå)", value=False)


# -----------------------------
# Load data
# -----------------------------
uploaded_files = st.file_uploader(
    "‡∏ß‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà",
    type=["csv"],
    accept_multiple_files=True,
)

dfs = []
issues_all = []

if use_sample and not uploaded_files:
    # Build a tiny synthetic sample for demo
    rng = np.random.default_rng(42)
    dates = pd.date_range("2025-08-01", periods=10, freq="D").date
    hours = list(range(9, 19))  # 9AM-6PM
    employees = [
        ("E001", "Alice"),
        ("E002", "Bob"),
        ("E003", "Chai"),
        ("E004", "Dao"),
    ]
    rows = []
    for d in dates:
        for h in hours:
            for eid, nm in employees:
                # Randomly sparse
                if rng.random() < 0.8:
                    cnt = int(rng.poisson(8))
                else:
                    cnt = 0
                rows.append([d, h, eid, nm, cnt])
    sample = pd.DataFrame(rows, columns=REQUIRED_COLS)
    df0, issues = coerce_schema(sample)
    df0 = deduplicate_rows(df0)
    dfs.append(df0)
    issues_all += issues

for f in uploaded_files or []:
    try:
        raw = read_csv_safely(f)
        df0, issues = coerce_schema(raw)
        df0 = deduplicate_rows(df0)
        dfs.append(df0)
        issues_all += issues
    except Exception as e:
        st.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå {getattr(f, 'name', 'file')}: {e}")

if not dfs:
    st.info("üëÜ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå CSV ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô **‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á** ‡πÉ‡∏ô‡πÅ‡∏ñ‡∏ö‡∏î‡πâ‡∏≤‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô")
    st.stop()

df = pd.concat(dfs, ignore_index=True)

# Report issues (if any)
if issues_all:
    with st.expander("‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"):
        for msg in issues_all:
            st.warning(msg)

# Basic validation
missing_cols = [c for c in REQUIRED_COLS if c not in df.columns]
if missing_cols:
    st.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ: ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô: {missing_cols}")
    st.stop()

# Optional filters
if date_filter_on and "Time (day)" in df.columns:
    min_d = pd.to_datetime(df["Time (day)"]).min()
    max_d = pd.to_datetime(df["Time (day)"]).max()
    d1, d2 = st.slider(
        "‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà",
        min_value=min_d.to_pydatetime().date(),
        max_value=max_d.to_pydatetime().date(),
        value=(min_d.to_pydatetime().date(), max_d.to_pydatetime().date()),
    )
    mask = (pd.to_datetime(df["Time (day)"]) >= pd.to_datetime(d1)) & (
        pd.to_datetime(df["Time (day)"]) <= pd.to_datetime(d2)
    )
    df = df.loc[mask].copy()

if employee_search.strip():
    mask = df["Name"].str.contains(employee_search.strip(), case=False, na=False)
    df = df.loc[mask].copy()

# Compute aggregates
emp_hourly, emp_daily, emp_summary = compute_aggregates(
    df, count_hour_when_processed_gt_zero=count_when_gt_zero
)

# KPIs
total_works = float(emp_daily["Works"].sum()) if not emp_daily.empty else 0.0
total_hours = float(emp_daily["Hours"].sum()) if not emp_daily.empty else 0.0
avg_wph = total_works / total_hours if total_hours > 0 else np.nan
employees = emp_summary.shape[0]
days = emp_daily["Time (day)"].nunique() if "Time (day)" in emp_daily.columns else 0

st.subheader("‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°")
kpi_block(total_works, total_hours, avg_wph, employees, days)

with st.expander("‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö", expanded=False):
    st.dataframe(df.head(100))

st.divider()

# Tabs
tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(
    ["üèÜ ‡∏•‡∏µ‡∏î‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ö‡∏≠‡∏£‡πå‡∏î", "üìÖ ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤", "üë§ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏£‡∏≤‡∏¢‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•", "‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥", "üìä ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ (Pivot)", "üì• ‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"]
)

with tab1:
    st.subheader("‡∏•‡∏µ‡∏î‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ö‡∏≠‡∏£‡πå‡∏î")
    st.caption(f"‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥ **{min_hours_threshold}** ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö WPH")
    if not emp_summary.empty:
        # Apply min-hours for WPH
        eligible = emp_summary[emp_summary["Hours"] >= min_hours_threshold].copy()
        col1, col2, col3 = st.columns(3, gap="large")
        with col1:
            add_topn_bar(emp_summary, "Works", top_n, "‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î")
        with col2:
            if eligible.empty:
                st.info("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö WPH")
            else:
                add_topn_bar(eligible, "WPH", top_n, "‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ï‡∏≤‡∏°‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á (WPH)")
        with col3:
            add_topn_bar(emp_summary, "Hours", top_n, "‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ï‡∏≤‡∏°‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô")

        st.markdown("---")
        st.subheader("‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î‡∏ï‡∏≤‡∏° WPH (‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥)")
        if not eligible.empty:
            bottom = eligible.sort_values("WPH", ascending=True).head(top_n)
            fig = px.bar(bottom, x="Name", y="WPH", hover_data=["Employee ID"], title="‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î‡∏ï‡∏≤‡∏° WPH")
            st.plotly_chart(fig, use_container_width=True)
            st.dataframe(bottom.reset_index(drop=True))
        else:
            st.info("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î‡∏ï‡∏≤‡∏° WPH")

with tab2:
    st.subheader("‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤")
    if emp_hourly.empty:
        st.info("‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤")
    else:
        # Hour-of-day throughput
        hourly = emp_hourly.groupby("Hour", as_index=False)["Processed Count"].sum()
        fig1 = px.line(hourly, x="Hour", y="Processed Count", markers=True, title="‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ô")
        st.plotly_chart(fig1, use_container_width=True)

        # Day-of-week & hour heatmaps
        thr, cov = heatmap_data(emp_hourly)
        if not thr.empty:
            st.markdown("**‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡πâ‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏á‡∏≤‡∏ô (‡∏ß‡∏±‡∏ô‡πÉ‡∏ô‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå √ó ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á)**")
            fig2 = px.imshow(thr, aspect="auto", title="‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡πâ‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏á‡∏≤‡∏ô", labels=dict(color="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏≤‡∏ô"))
            st.plotly_chart(fig2, use_container_width=True)

        if not cov.empty:
            st.markdown("**‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡πâ‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ó‡∏≥‡∏á‡∏≤‡∏ô (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô)**")
            fig3 = px.imshow(cov, aspect="auto", title="‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡πâ‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ó‡∏≥‡∏á‡∏≤‡∏ô", labels=dict(color="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô"))
            st.plotly_chart(fig3, use_container_width=True)

        # Daily totals
        daily_total = emp_daily.groupby("Time (day)", as_index=False)["Works"].sum()
        fig4 = px.bar(daily_total, x="Time (day)", y="Works", title="‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°‡∏á‡∏≤‡∏ô‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô")
        st.plotly_chart(fig4, use_container_width=True)

        # Peak hours table
        st.markdown("**‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏á‡∏≤‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î**")
        peak_hours = (
            emp_hourly.groupby("Hour", as_index=False)["Processed Count"].sum().sort_values("Processed Count", ascending=False)
        )
        st.dataframe(peak_hours.head(24))

with tab3:
    st.subheader("‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏£‡∏≤‡∏¢‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•")
    if emp_summary.empty:
        st.info("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô")
    else:
        names = emp_summary["Name"].tolist()
        picked = st.selectbox("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô", options=names)
        if picked:
            # Determine the employee ID (in case of duplicate names, show all matches)
            emp_ids = emp_summary.loc[emp_summary["Name"] == picked, "Employee ID"].unique().tolist()
            if len(emp_ids) > 1:
                emp_id = st.selectbox("‡∏û‡∏ö‡∏´‡∏•‡∏≤‡∏¢ ID ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏µ‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£:", options=emp_ids)
            else:
                emp_id = emp_ids[0]

            dfd = emp_daily[(emp_daily["Name"] == picked) & (emp_daily["Employee ID"] == emp_id)].copy()
            if dfd.empty:
                st.info("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å")
            else:
                c1, c2, c3 = st.columns(3)
                works_total = int(dfd["Works"].sum())
                hours_total = int(dfd["Hours"].sum())
                wph = works_total / hours_total if hours_total > 0 else np.nan
                c1.metric("‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", f"{works_total:,}")
                c2.metric("‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", f"{hours_total:,}")
                c3.metric("WPH (‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°)", f"{wph:,.2f}" if not np.isnan(wph) else "‚Äî")

                fig = px.line(dfd, x="Time (day)", y="Works", markers=True, title=f"‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏á‡∏≤‡∏ô‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô ‚Äî {picked}")
                st.plotly_chart(fig, use_container_width=True)

                fig_wph = px.line(dfd, x="Time (day)", y="WPH", markers=True, title=f"WPH ‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô ‚Äî {picked}")
                st.plotly_chart(fig_wph, use_container_width=True)

                # Hourly profile of this employee
                base = emp_hourly[(emp_hourly["Name"] == picked) & (emp_hourly["Employee ID"] == emp_id)].copy()
                prof = base.groupby("Hour", as_index=False)["Processed Count"].sum()
                fig_prof = px.bar(prof, x="Hour", y="Processed Count", title=f"‡πÇ‡∏õ‡∏£‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏≤‡∏¢‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á ‚Äî {picked}")
                st.plotly_chart(fig_prof, use_container_width=True)

                st.markdown("**‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô)**")
                st.dataframe(dfd.sort_values("Time (day)"))

with tab4:
    st.subheader("‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥")
    if emp_daily.empty:
        st.info("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥")
    else:
        z_thr = st.slider("‡πÄ‡∏Å‡∏ì‡∏ë‡πå Z-score", 1.0, 4.0, 2.0, 0.5)
        anom = anomaly_report(emp_daily, z_threshold=z_thr)
        flagged = anom[anom["IsAnomaly"]].copy()
        st.caption("‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏≤‡∏ô' ‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ß‡∏±‡∏ô‡∏Ç‡∏≠‡∏á‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡πÄ‡∏ö‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏ö‡∏ô‡πÑ‡∏õ‡∏à‡∏≤‡∏Å‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á‡∏ï‡∏ô‡πÄ‡∏≠‡∏á ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö‡∏Ñ‡πà‡∏≤ Z-score ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î")
        st.dataframe(flagged.sort_values(["Name", "Time (day)"]))
        if not flagged.empty:
            by_emp = flagged.groupby("Name").size().reset_index(name="Anomaly Days")
            fig = px.bar(by_emp, x="Name", y="Anomaly Days", title="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏ô")
            st.plotly_chart(fig, use_container_width=True)

with tab5:
    st.subheader("‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ (Pivot)")
    if emp_hourly.empty:
        st.info("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ")
    else:
        # Daily x Employee pivot of Works
        piv = (
            emp_hourly.groupby(["Time (day)", "Employee ID", "Name"], as_index=False)["Processed Count"].sum()
            .pivot_table(index="Time (day)", columns="Name", values="Processed Count", aggfunc="sum")
            .fillna(0)
        )
        st.markdown("**‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô √ó ‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô ‚Äî ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏≤‡∏ô**")
        st.dataframe(piv)

        # Hour x Employee pivot (sum)
        piv2 = (
            emp_hourly.groupby(["Hour", "Employee ID", "Name"], as_index=False)["Processed Count"].sum()
            .pivot_table(index="Hour", columns="Name", values="Processed Count", aggfunc="sum")
            .fillna(0)
        )
        st.markdown("**‡∏£‡∏≤‡∏¢‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á √ó ‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô ‚Äî ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏á‡∏≤‡∏ô**")
        st.dataframe(piv2)

with tab6:
    st.subheader("‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
    st.caption("‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV")

    c1, c2, c3 = st.columns(3)
    with c1:
        download_csv_button(emp_summary, "employee_summary.csv", "‚¨áÔ∏è ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô (‡∏£‡∏≤‡∏¢‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•)")
    with c2:
        download_csv_button(emp_daily, "employee_daily.csv", "‚¨áÔ∏è ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô (‡∏£‡∏≤‡∏¢‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•‡∏ï‡πà‡∏≠‡∏ß‡∏±‡∏ô)")
    with c3:
        download_csv_button(emp_hourly, "employee_hourly.csv", "‚¨áÔ∏è ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏¢‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á")

    st.markdown("---")
    st.markdown("**‡πÄ‡∏Ñ‡∏•‡πá‡∏î‡∏•‡∏±‡∏ö:** ‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ü‡∏•‡πå CSV ‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡πÄ‡∏î‡∏∑‡∏≠‡∏ô")

st.caption("‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏î‡πâ‡∏ß‡∏¢ ‚ù§Ô∏è ‚Äî ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ß‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå CSV ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÑ‡∏î‡πâ‡∏ï‡∏•‡∏≠‡∏î‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏µ‡πÄ‡∏ü‡∏£‡∏ä‡πÅ‡∏î‡∏ä‡∏ö‡∏≠‡∏£‡πå‡∏î")
